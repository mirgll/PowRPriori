---
title: "Power Analysis with PowRPriori: A Complete Workflow with Different Examples"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Complete Workflow with PowRPriori}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: references.bib
link-citations: yes
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Introduction

Welcome to `PowRPriori`! A priori power analysis is a critical step in designing robust and efficient studies. This package is designed to make power analysis for complex linear models (LMs, LMMs, GLMs, and GLMMs) intuitive, user-friendly and accessible. It ensures robust estimations by using Monte-Carlo-style data simulation as a foundation for the power analysis. One of the main goals of the package is to provide a framework for a priori power analyses which does not necessitate a deep understanding of the underlying statistics or ins and outs of the model specifications. Ideally, you can focus on your research design and expected effect sizes with the package doing the rest for you without overly complicated function parameters.

This vignette will guide you through the entire workflow of a power analysis using a realistic example. It will cover:
1.  Defining a study design.
2.  Specifying expected effects.
3.  Running the power simulation.
4.  Interpreting and visualizing the results.

Finally, it will show brief examples of how to adapt the workflow for other common study designs.

```{r load-packages, message=FALSE}
# First, let's load PowRPriori and other helpful packages
library(PowRPriori)
library(tidyr) # For creating the means table
```

## A Complete Example: A 2x2 Mixed Design

Let's imagine a common research scenario: You want to test a new cognitive training program. 

**Research Question:** Does your new training program (`group`: Treatment vs. Control) improve memory scores from a pre-test to a post-test (`time`) more than in a control group?

This is a classic 2x2 mixed design, with `group` as a between-subject factor and `time` as a within-subject factor. Since `time` is a within-subject factor, it can be seen as "nested" within each individual measured, making your research design well suited to be analyzed using a linear mixed effects model. The important questions for your power analysis are, what does your study design look like, what are the effect sizes you can realistically expect (based on the existing literature) and what are reasonable estimates for the random effects structure?

### Step 1: Defining the Study Design

First, you need to translate your study design into a `PowRPriori_design` object. You need to specify the name of your subject ID (the `id`-parameter, in your case `subject`), your between-subject factors, and your within-subject factors. The `id` parameter always represents the analysis unit on the lowest level of your mixed effects design, and does not necessarily have to be a person (it could also be e.g. a plot in a garden).

```{r define-design}
my_design <- define_design(
  id = "subject",
  between = list(group = c("Control", "Treatment")),
  within = list(time = c("pre", "post"))
)
```

### Step 2: Defining the Statistical Model

Next you need to translate your research question into a testable statistical model. This package uses lme4-style formulas to do that. The research question of the example used here implies an interaction effect, since you want to know if the effect of `time` is different between the Control and the Treatment groups. You also need to include a random intercept for `subject` to account for the fact that measurements from the same person are correlated (representing the "nesting" of the data).

Your lme4-style model formula should look like this: `score ~ group * time + (1 | subject)`

On the left hand side of the tilde is your outcome variable and on the right hand side your combination of fixed (`group * time`) and random (`(1 | subject)`) effects. 

These next two subsections will feature very brief, non exhaustive tutorials on lme4-style formulas, to aid in correctly defining the myriad possible fixed and random effects structures. For a more detailed description, you can look up @JSSv067i01.

#### Fixed effects in lme4-formulas

In theory, you can put together any number of fixed effects in lme4-style formulas via the `+`-sign. For example, a formula analyzing the influence of several fixed effects factors on an outcome could look like so: `outcome ~ factor1 + factor2 + factor3 + ... + factorX`. Now, such a formula assumes that none of the specified factors are associated in any way. 

There are however instances where you might wish to analyze whether the influence of one factor on the outcome is depending on another factor in some way - i.e. you want to analyze a potential interaction effect. Interaction effects are included in lme4-style formulas via the `:` sign (e.g. `outcome ~ factor1:factor2`). If you include only `factor1:factor2` in your model formula, lme4 will explicitly only estimate the interaction term in your model, but not the main effects. To include the main as well as the interaction effects, you need to write `outcome ~ factor1 + factor2 + factor1:factor2`. 

lm4-style formulas also offer a shortcut for writing both the main and interaction effect. If you write e.g. `outcome ~ factor1*factor2`, lme4 will automatically parse the formula to `outcome ~ factor1 + factor2 + factor1:factor2`. If you want to include a factor that is not part of the interaction, you can simply add it by using `+`, e.g. `outcome ~ factor1*factor2 + factor3`.

#### Random effects in lme4-formulas

In lme4-style formulas, random effects are always enclosed in parentheses and simply added to the formula via a `+`-sign. What is special about the random effects in lme4-style formulas is the `|`-operator. In easy terms, you can think of the parameters on the right side of the `|` as the random intercepts you want to specify, and the parameters on the left hand side of the `|` as the random slopes. If you only want to include a random intercept in your formula, you would write `(1 | factor2)`. You can also specify multiple random intercepts by adding more than one random effects term, e.g. `(1 | factor2) + (1 | factor3)`. 

In cases where one factor is nested within another one, i.e. each sub-group belongs exclusively to a single parent group (e.g. classes within schools), you can use `/` to specify such a relationship. For example, if you specify `(1 | factor1/factor2)` as your random effects structure, it would be interpreted such that `factor2` would be considered to be nested within `factor1`.

Parameters that should be treated as having random slopes are specified on the left-hand side of the `|`-operator. For example, `(factor1 | factor2)` is read such that `factor2` is treated as a random intercept, and `factor1` is treated as having a random slope, and is correlated with `factor2`. If you wish to signify that the random slope should be treated as uncorrelated with the random intercept, this would be specified by using `||` instead of `|`, e.g. `(factor1 || factor2)`.

### Step 3: Specifying the Hypothesized Effects

This is the most critical part of your power analysis. You need to define the effect sizes (i.e. the regression coefficients in the case of (G)LMMs) which you expect in our study. Ideally, you can use existing research to guide the decision-making process for finding appropriate effect sizes. If you cannot base your effect sizes on previous research, it should be solidly grounded in theory. In any case, the chosen effect sizes for which you run your power analyses should always be appropriately justified. For more details, refer to @samplesize or @equivalence.

You could, however, also simulate however many effect sizes you like in a more iterative process. In fact, varying the effect sizes can be a good idea to get more robust estimates and a general feel for your simulated data. In order to correctly simulate the data and, more importantly, fit the model, the names of the coefficients you plug into `PowRPRiori` need to be specified exactly as the model fitting engine (`lme4` in the current release version of this package) expects them. `PowRPriori` makes this process easy, and gives you two options to accomplish it.

First, if you already know the values of the coefficients, you can use the `get_fixed_effects_structure` helper function to let `PowRPriori` automatically print a ready-to-use code snippet to the console containing all correctly named coefficients. This output is already structured so that `PowRPriori` understands it. The values themselves are left as placeholders (`...`), so all you need to do is fill them with values. Second, if you do not know the values of the coefficients, `PowRPRiori` also allows you to think in terms of expected mean outcomes and calculates the necessary coefficients for you. Let's go through both scenarios.

##### Scenario A: You do not know the values of the coefficients

Let's start with the case where you do not know the values of the coefficients, but can estimate reasonable values for the mean outcome scores in each condition.

For now, let's also assume the memory score is on a 100-point scale and hypothesize the following mean scores for each condition:

-The **Control** group starts at 50 and improves slightly to 52 (a small practice effect).

-The **Treatment** group also starts at 50 but improves significantly to 60 due to the intervention.

You first need to create a data frame containing these expected means, which you then provide to the `PowRPRiori`-function that calculates the regression coefficients from them:

```{r specify-means}
expected_means <- tidyr::expand_grid(
  group = c("Control", "Treatment"),
  time = c("pre", "post")
)

# Assign the means based on our hypothesis
expected_means$mean_score <- c(50, 52, 50, 60)

knitr::kable(expected_means, caption = "Expected Mean Scores for Each Condition")
```
In more complex examples, the sequence of the means might not be as straightforward as in this case. In such cases you can look at the resulting data frame created by `expand_grid` to see the generated sequence of conditions. The sequence of the mean values in your vector (`expected_means$mean_score <- c(50, 52, 50, 60)` above) needs to correspond correctly to the conditions in your `expected_means` data frame.

Now that you have the data frame containing the expected mean outcomes, you can use the helper function `fixed_effects_from_average_outcome()` to translate these intuitive means into the regression coefficients your model needs.

```{r get-fixed-effects}
my_fixed_effects <- fixed_effects_from_average_outcome(
  formula = score ~ group * time,
  outcome = expected_means
)

#Note the naming of the coefficients is exactly as `lme4` expects them to be. Do not change these names!
print(my_fixed_effects)
```

##### Scenario B: You already know the values of the coefficients

Let's now look at a scenario where you already know the values of the coefficients. Other than in the former scenario, the assumption here is that you already knew (or could estimate) the regression coefficient values produced by the `fixed_effects_from_average_outcome` function beforehand. In this case, you can use the `get_fixed_effects_structure` helper function to generate a code snippet where you only need to plug in the regression coefficients. `get_fixed_effects_structure` uses the formula of your model in combination with your `PowRPriori_design`-object (`my_design` from above) to generate the code snippet:

```{r get fixed effects structure}
get_fixed_effects_structure(formula = score ~ group * time + (1 | subject), design = my_design)

```

Now we can fill in the values:
```{r}
my_fixed_effects <- list(
   `(Intercept)` = 50,
   groupTreatment = 0,
   timepost = 2,
   `groupTreatment:timepost` = 8
)
```


### Step 4: Specifying the Random Effects

You also need to define the values of the random effects structure (i.e. the variance components) in your model. Again, ideally, you have some sort of guide for the specific values of these. The goal should always be to supply plausible, realistic values here, guided by e.g. pre-existing (optimally from an independent sample) data or at the very least solid theoretical considerations.

The random effects in your model can influence the fitting process considerably when using mixed effects models, especially in more complex scenarios. This means that you'll need to strike a balance between realistic values and tweaking them so that the model can be fit in the first place. Fortunately, `PowRPriori` lets you know if the model fitting process encounters problems during the simulation.

For now, let's assume the standard deviation of your subjects' baseline scores (the random intercept) is 8 points, and the residual standard deviation (random noise) unexplained by our model is 12 points.

You can use `get_random_effects_structure()` to get a template very similar to what `get_fixed_effects_structure()` provides.

```{r get-random-effects}
# This helps you get the correct names
get_random_effects_structure(score ~ group * time + (1|subject), my_design)

```

Now you can again fill in the values:
```{r specify-random-effects}
my_random_effects <- list(
  subject = list(
    `(Intercept)` = 8
  ),
  sd_resid = 12
)
```

### Step 5: Run the Simulation

You now have all the ingredients to run a proper power simulation! The function for this, which can be considered the heart of the package, is `power_sim`.

Let's say you want to find the sample size needed to achieve 80% power at an alpha level of 5% for your key hypothesis: the `groupTreatment:timepost` interaction (specified via the `test_parameter` parameter). We need to test this interaction since our research question was whether our training program improves memory scores more than they change in a control group where no treatment is applied. 

An important note though: the 80% power and 5% alpha level are merely conventions - they can be used, but should not be considered set in stone. Depending on the context, you might also want to adjust these.

Let's start the simulation with a sample size of N=30 (the `n_start` parameter) and increase it in steps of 10 (`n_increment` parameter). The example here uses a fairly low number of simulated data sets for each sample size (the `n_sims` parameter) - in real simulations, you should use a much larger number (at least 1000, default for the `power_sim` function is 2000). Again, the 1000 simulations are not set in stone and shouldn't be considered anything else than a rough rule of thumb, the number is merely used to illustrate what might be considered a large enough number. A low number of simulations increases the risk "bad draws" (random samples that are not representative / outliers for the simulated population) influencing the results of the power analysis to a high degree and should thus be avoided. 

```{r run-simulation, eval=FALSE}
# NOTE: This function can take a few minutes to run, depending on model complexity.

power_results <- power_sim(
  formula = score ~ group * time + (1 | subject),
  design = my_design,
  fixed_effects = my_fixed_effects,
  random_effects = my_random_effects,
  test_parameter = "groupTreatment:timepost",
  n_start = 30,
  n_increment = 10,
  n_sims = 200, # Use >= 1000 for real analysis
  power_crit = 0.80,
  alpha = 0.05,
  parallel_plan = "sequential"
)
```

### Step 6: Interpret and Visualize the Results

After the simulation is complete, you can inspect the results object in different ways.

```{r load-results, include=FALSE, eval=TRUE}
# This hidden code block loads the pre-computed results
power_results <- readRDS(
  system.file("extdata", "power_results_vignette.rds", package = "PowRPriori")
)
```

#### The Summary Table

The `summary()` function provides a detailed overview of the simulation, including the power table and a check of the parameter recovery (i.e. a check how well the simulation was able to simulate the parameters you specified, with bias values representing deviations from your specified parameters). The parameter recovery largely serves to confirm that the simulation ran as expected and that the data generated is plausible.

```{r summary}
summary(power_results)
```
From this table, you can see that you'll need approximately **N=140 subjects** to reach your desired 80% power at an alpha level of 5%. Also, you can see that your model had some singular fits, especially at smaller sample sizes. Sometimes, singular fits can happen by chance. If they happen frequently, it can be an indicator that your model specifications might be problematic. Right now, I won't delve further into this topic, but investigating your random effects parameters is useful in such cases. 

#### The Power Curve

A plot of the power curve is often an intuitive way to present the overall trajectory of the power across the increasing sample sizes.

```{r plot-power-curve, fig.width=6, fig.height=4}
plot_sim_model(power_results, type = "power_curve")
```

This plot visually confirms your findings from the table.

#### Plotting sample data

Another useful aspect to investigate is whether the data your simulation produced looks plausible or conforms to your initial intentions. To investigate this, you can call the `plot_sim_model` function with the `type`-parameter set to`"data"` to plot sample data from the last simulation run of, which is saved in your `PowRPriori` object.

```{r, fig.width=6, fig.height=4}
plot_sim_model(power_results, type = "data")
```



## Other Use Cases

The `PowRPriori` workflow is flexible. Here are brief examples for other common designs.

### Use Case 1: A Cluster-Randomized Trial

With `PowRPriori`, you can also define nested designs. 
You could also design your study so that the intervention is applied to whole classes, not individual pupils. You can use the hierarchical structure in `define_design` to specify this. In the following scenario, the intervention is applied at class level, i.e. if a class is in the intervention group, every pupil in the respective class is also in the intervention group.

```{r cluster-design 1}
cluster_design <- define_design(
  id = "pupil",
  nesting_vars = list(class = 1:20), # 20 classes
  between = list(
    class = list(intervention = c("yes", "no")) # Intervention at class level
  ),
  within = list(
    time = c("pre", "post")
  )
)

# The rest of the workflow (specifying effects, running power_sim)
# would follow the same logic as the main example.
```
### Use Case 2: Logistic Regression (GLMM)

If your outcome is binary (e.g., pass/fail), you can specify `family = "binomial"`. You would then specify your expected effects as **probabilities** (between 0 and 1). 
You can also simulate count data in a similar way by specifying `familty = "poisson"` and supplying the means for `fixed_effects_from_average_outcome()` as mean counts.

```{r glmm-example}
# You could for example expect the Control group to have a 50% pass rate at both times.
# The Treatment group starts at 50% but improves to a 75% pass rate.
glmm_probs <- expand_grid(
  group = c("Control", "Treatment"),
  time = c("pre", "post")
)
glmm_probs$pass_prob <- c(0.50, 0.50, 0.50, 0.75)

# The fixed effects are calculated from these probabilities
glmm_fixed_effects <- fixed_effects_from_average_outcome(
  formula = passed ~ group * time,
  outcome = glmm_probs,
  family = "binomial"
)

# Note: For binomial (and poisson) models, sd_resid is not specified in random_effects. You can also use generate_random_effects_structure as detailed before.
glmm_random_effects <- list(
  subject = list(Intercept = 2.0)
)

# The power_sim() call would then include `family = "binomial"` (or `family = "poisson"` if you simulated count data).
```

### Use Case 3: Using Intraclass Coefficients for the Random Effects Structure

Alternatively to specifying the random effects directly, `PowRPriori` also allows you to do this via Intraclass-Coefficients (ICCs), as long as your model only includes random intercepts. This changes the function call to `power_sim` slightly. When using Intraclass-Coefficients, the `random_effects`-parameter is omitted and the `icc_specs`- in combination with the `overall_variance`-parameter are used. Thus, you need to specify the Intraclass-Coefficients for all random intercepts in your model as well as the overall variance expected in the data. Since you don't need to specify the `random_effects`-parameter in these cases, there is also no use for the `get_random_effects_structure()`-function here. 

Here is a brief example of the workflow using Intraclass-Coefficients:
```{r ICC example, eval=FALSE}
my_icc_design <- define_design(
  id = "subject",
  between = list(group = c("Control", "Treatment")),
  within = list(time = c("pre", "post"))
)

#Note: Only random intercept models work with the ICC specification
my_icc_formula <- score ~ group * time + (1 | subject)

get_fixed_effects_structure(formula = my_icc_formula, design = my_icc_design)

my_icc_fixed_effects <- list(
   `(Intercept)` = 50,
   groupTreatment = 0,
   timepost = 2,
   `groupTreatment:timepost` = 8
)

#The values are defined so they mirror the random effects structure from the detailed example above
iccs <- list(`subject` = 0.4)
overall_var <- 20

power_results <- power_sim(
  formula = score ~ group * time + (1 | subject),
  design = my_design,
  fixed_effects = my_fixed_effects,
  icc_specs = iccs,
  overall_variance = overall_var,
  test_parameter = "groupTreatment:timepost",
  n_start = 30,
  n_increment = 10,
  n_sims = 200, 
  power_crit = 0.80,
  alpha = 0.05,
  parallel_plan = "sequential"
)
```

The summary / visualization functions can be used in the same way as detailed above. 

## Conclusion

This vignette demonstrated the complete workflow for conducting a power analysis with `PowRPriori`. By focusing on a clear definition of the design and an intuitive specification of the expected effects, the package aims to make power analysis via Monte-Carlo-style data simulation a straightforward and more robust process.

## References
